{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "xtW1_Yer9ffz",
        "K3Xh3JjxK2WE"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3dab9f6f5fe04b3d99fcb1c79d84fbaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b6174bfef70e464aa30d08dbb5ece52b",
              "IPY_MODEL_1aa3b4fcaf004342963fcfcfab08cc05",
              "IPY_MODEL_0a8e3e28d1d1434ba85a545050553d2f"
            ],
            "layout": "IPY_MODEL_c6bfe63f35f042008934e5c6cdbd7c79"
          }
        },
        "b6174bfef70e464aa30d08dbb5ece52b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df42d523e5134e1db275fd1edc7b8025",
            "placeholder": "​",
            "style": "IPY_MODEL_affba0ad31af4c7f9da02fe2ed26536e",
            "value": "Generating train split: "
          }
        },
        "1aa3b4fcaf004342963fcfcfab08cc05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b183eef5a398432494fac1635013a906",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c0f77e5a64e14b6684f326a2d3e89db4",
            "value": 1
          }
        },
        "0a8e3e28d1d1434ba85a545050553d2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_503023604737469ea39d99e4c3e24129",
            "placeholder": "​",
            "style": "IPY_MODEL_61e9e124318740419f8d0f834c47dc5d",
            "value": " 2797/0 [00:00&lt;00:00, 14680.59 examples/s]"
          }
        },
        "c6bfe63f35f042008934e5c6cdbd7c79": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df42d523e5134e1db275fd1edc7b8025": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "affba0ad31af4c7f9da02fe2ed26536e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b183eef5a398432494fac1635013a906": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "c0f77e5a64e14b6684f326a2d3e89db4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "503023604737469ea39d99e4c3e24129": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61e9e124318740419f8d0f834c47dc5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "82f52c6d37e34e279d31d6736ffb9912": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_92bc8af216964e43a6b71abc17b6d0d2",
              "IPY_MODEL_35f38f3a20384d0aafe09a3adef6240e",
              "IPY_MODEL_92a94c986d254fd3aee54c9fa3821a97"
            ],
            "layout": "IPY_MODEL_5a60bad4641a4909a746c5ea149e9d04"
          }
        },
        "92bc8af216964e43a6b71abc17b6d0d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e9ab4f0a2c9d4a5d9863ee2ac2953424",
            "placeholder": "​",
            "style": "IPY_MODEL_687e5768f3f445a29dc4a3044ad2b20d",
            "value": "Map: 100%"
          }
        },
        "35f38f3a20384d0aafe09a3adef6240e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c8811f4711642c5a3cc405fbca5db55",
            "max": 2797,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7518ed7295054344a7b31ac9f26131d5",
            "value": 2797
          }
        },
        "92a94c986d254fd3aee54c9fa3821a97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d11ad1d799f34437a5e6812784585981",
            "placeholder": "​",
            "style": "IPY_MODEL_1adcc21163894567b647cde13b8dc025",
            "value": " 2797/2797 [00:00&lt;00:00, 31269.07 examples/s]"
          }
        },
        "5a60bad4641a4909a746c5ea149e9d04": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9ab4f0a2c9d4a5d9863ee2ac2953424": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "687e5768f3f445a29dc4a3044ad2b20d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1c8811f4711642c5a3cc405fbca5db55": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7518ed7295054344a7b31ac9f26131d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d11ad1d799f34437a5e6812784585981": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1adcc21163894567b647cde13b8dc025": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6ddae0ae9ebd492c9ab6b6665ebdade5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7303f70ab7664c6ab11fbb1b5d2ed7c6",
              "IPY_MODEL_d5639424c4bc4ab4a44665fc879bdede",
              "IPY_MODEL_b21076cfc6f64d67baed12bc1705054d"
            ],
            "layout": "IPY_MODEL_6d04ab098f304ea082d8617b6cd269a7"
          }
        },
        "7303f70ab7664c6ab11fbb1b5d2ed7c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d24f5ab4bffb4ab89ee3e896128e76b5",
            "placeholder": "​",
            "style": "IPY_MODEL_7a8e96e146e647b8beb955dcfa4e0b03",
            "value": "Map: 100%"
          }
        },
        "d5639424c4bc4ab4a44665fc879bdede": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_60b4278b12a54c8690569b11bfb87c3b",
            "max": 2797,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5747392fbba74580881f6f29e795a4fd",
            "value": 2797
          }
        },
        "b21076cfc6f64d67baed12bc1705054d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd36d636bb204c1d9afadaed8b563521",
            "placeholder": "​",
            "style": "IPY_MODEL_829992699f9d48a1874ee651194d3db7",
            "value": " 2797/2797 [00:01&lt;00:00, 1785.88 examples/s]"
          }
        },
        "6d04ab098f304ea082d8617b6cd269a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d24f5ab4bffb4ab89ee3e896128e76b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a8e96e146e647b8beb955dcfa4e0b03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "60b4278b12a54c8690569b11bfb87c3b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5747392fbba74580881f6f29e795a4fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cd36d636bb204c1d9afadaed8b563521": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "829992699f9d48a1874ee651194d3db7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0c92fdcdaee7489397f59e116c2443ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_53e183a626ca4aeabc2bd275ee45748a",
              "IPY_MODEL_0a3f7f12390341d195baf76cf6bf5d06",
              "IPY_MODEL_91f56f5726db44ffb19039e8e7da9ed6"
            ],
            "layout": "IPY_MODEL_ee78faea95e04c31886d366c3427d39a"
          }
        },
        "53e183a626ca4aeabc2bd275ee45748a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ace044b556ff4d80b65f48a339937b28",
            "placeholder": "​",
            "style": "IPY_MODEL_9e5e880a3a524507a14967fec2795549",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "0a3f7f12390341d195baf76cf6bf5d06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e179147b970f4cb9a1e466b94fc65897",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d76a78cdf4c34771b72dfe29942024e0",
            "value": 2
          }
        },
        "91f56f5726db44ffb19039e8e7da9ed6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc6977621129492bb4b0b95f94e495d7",
            "placeholder": "​",
            "style": "IPY_MODEL_88aa7ef4b9df4a4299681017560f2519",
            "value": " 2/2 [00:26&lt;00:00, 11.21s/it]"
          }
        },
        "ee78faea95e04c31886d366c3427d39a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ace044b556ff4d80b65f48a339937b28": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e5e880a3a524507a14967fec2795549": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e179147b970f4cb9a1e466b94fc65897": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d76a78cdf4c34771b72dfe29942024e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cc6977621129492bb4b0b95f94e495d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88aa7ef4b9df4a4299681017560f2519": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "35bb7332ab7f476ea5db7a29c437c307": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5c9b013686dd435084bbbb958bed45fc",
              "IPY_MODEL_6b846ad868fd496c97e392726cfeb8d6",
              "IPY_MODEL_8bf37d29c3444a6c9d7491f7f630da56"
            ],
            "layout": "IPY_MODEL_01e1f6d58b584ac19169b60707012b80"
          }
        },
        "5c9b013686dd435084bbbb958bed45fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8012a9653e83459793a3d237434e0bba",
            "placeholder": "​",
            "style": "IPY_MODEL_f98463c9c56a446689eade305ea24c32",
            "value": "Map: 100%"
          }
        },
        "6b846ad868fd496c97e392726cfeb8d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_95e51857038f4000a9270fc66fb24dac",
            "max": 5602,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_967f6508e2eb4649bde0c79e10b68034",
            "value": 5602
          }
        },
        "8bf37d29c3444a6c9d7491f7f630da56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cea6e832b0ce413383a11bf3cc2413b8",
            "placeholder": "​",
            "style": "IPY_MODEL_cb49ff51997b48528f8fa15b4d1c1c22",
            "value": " 5602/5602 [00:00&lt;00:00, 44665.89 examples/s]"
          }
        },
        "01e1f6d58b584ac19169b60707012b80": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8012a9653e83459793a3d237434e0bba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f98463c9c56a446689eade305ea24c32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "95e51857038f4000a9270fc66fb24dac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "967f6508e2eb4649bde0c79e10b68034": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cea6e832b0ce413383a11bf3cc2413b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb49ff51997b48528f8fa15b4d1c1c22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "518c7c01bf1c45e8ab83abc06a060ab2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_28e86a00bf4c4199b8a4cf251ef91e32",
              "IPY_MODEL_196529ae977d4c5c9c027ce16e44fbcb",
              "IPY_MODEL_d69c6d50ea4f481ab0dbc1e01d3036bd"
            ],
            "layout": "IPY_MODEL_bbfa207b36524dcdb886534a0f42b88d"
          }
        },
        "28e86a00bf4c4199b8a4cf251ef91e32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4574c31aa8a844d5bbc4d8c71eb9180c",
            "placeholder": "​",
            "style": "IPY_MODEL_16c06a1144e14d2089e22b7c4ebfa7aa",
            "value": "Map: 100%"
          }
        },
        "196529ae977d4c5c9c027ce16e44fbcb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_943e5efa79fc4b989508311c48e4a5d1",
            "max": 5602,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_de95f3cb5f05446d841fe6034d41ea58",
            "value": 5602
          }
        },
        "d69c6d50ea4f481ab0dbc1e01d3036bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45ba5988a8664444870e3eb4467c0416",
            "placeholder": "​",
            "style": "IPY_MODEL_04dd3c3aae0d4d0aab91bd59676b00d2",
            "value": " 5602/5602 [00:02&lt;00:00, 2218.54 examples/s]"
          }
        },
        "bbfa207b36524dcdb886534a0f42b88d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4574c31aa8a844d5bbc4d8c71eb9180c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16c06a1144e14d2089e22b7c4ebfa7aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "943e5efa79fc4b989508311c48e4a5d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de95f3cb5f05446d841fe6034d41ea58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "45ba5988a8664444870e3eb4467c0416": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04dd3c3aae0d4d0aab91bd59676b00d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/npinto97/LLM_Fine-tuning/blob/main/NLPProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLP project a.y. 2023-24\n",
        "\n",
        "Nicolas Pinto 807348\n",
        "\n",
        "Emanuele Tanzi 807406"
      ],
      "metadata": {
        "id": "U6_hmtION5tA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM360: A Comprehensive Approach to Utilizing Large Language Models"
      ],
      "metadata": {
        "id": "V-pKpJdBB5fK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This work presents LLM360, an innovative project that explores the use of large language models (LLMs) through a complete cycle that includes dataset generation, model fine-tuning, and response evaluation. The primary goal is to assess the effectiveness of LLMs in various application contexts, improving their capabilities in generating explanations for Italian terms and studying the semantic evolution of words over time.\n",
        "\n",
        "In this study, we will generate a dataset of Italian terms and their diachronical explanations, fine-tune a pre-trained LLM to enhance its explanatory capabilities, and evaluate the quality of the generated explanations using both quantitative metrics and qualitative human assessments via an LLM. Through this integrated cycle, we aim to demonstrate the efficacy of LLMs in capturing and understanding the nuanced semantic evolution of words over time."
      ],
      "metadata": {
        "id": "YIalnmctB_la"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Generation\n",
        "\n",
        "In this section, we use the dataset provided by WiC-ITA (https://wic-ita.github.io/) as the foundation for constructing the synthetic dataset on which the fine-tuning will be based.\n",
        "\n",
        "WiC-ITA offers a dataset designed for the word sense disambiguation task, where the objective is to determine whether a word *w* that appears in two sentences *s1* and *s2* has the same meaning in both contexts.\n",
        "\n",
        "The dataset includes several attributes, such as the lemma of the word to be examined, the two sentences in which the word appears, and other attributes useful for the task.\n",
        "\n",
        "For our purposes, we extract the lemmas of all the words to be examined from both the training and test sets of the WiC-ITA dataset and save them into two separate files. It is important to note that, due to the nature of the task, some words in the test dataset also appear in the training dataset. Therefore, we removed all words from the test word list that were already present in the training word list."
      ],
      "metadata": {
        "id": "xtW1_Yer9ffz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install together langchain_together langchain_core"
      ],
      "metadata": {
        "id": "qdUeXnl9ABXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json"
      ],
      "metadata": {
        "id": "RSIuwHcBi4cb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract lemmas from the JSON file\n",
        "def extract_lemmas(input_file_path, output_file_path):\n",
        "    with open(input_file_path, 'r', encoding='utf-8') as input_file:\n",
        "        lines = input_file.readlines()\n",
        "\n",
        "    lemmas = []\n",
        "\n",
        "    for line in lines:\n",
        "        data = json.loads(line)\n",
        "        lemmas.append(data[\"lemma\"])\n",
        "\n",
        "    with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
        "        for lemma in lemmas:\n",
        "            output_file.write(lemma + '\\n')\n"
      ],
      "metadata": {
        "id": "hiof1TkW9-I7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_file_path = '/content/terms_sentences_train.jsonl'\n",
        "output_file_path = '/content/list_of_words_train.txt'\n",
        "\n",
        "extract_lemmas(input_file_path, output_file_path)\n"
      ],
      "metadata": {
        "id": "UTmhDkk73ph4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_file_path = '/content/terms_sentences_test.jsonl'\n",
        "output_file_path = '/content/list_of_words_test.txt'\n",
        "\n",
        "extract_lemmas(input_file_path, output_file_path)\n"
      ],
      "metadata": {
        "id": "tEcnt7Io-nK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Isolation of terms that appear in the test and not in the train\n",
        "with open('list_of_words_train.txt', 'r') as f1:\n",
        "    words_first_list = set(line.strip() for line in f1)\n",
        "\n",
        "with open('list_of_words_test.txt', 'r') as f2:\n",
        "    words_second_list = set(line.strip() for line in f2)\n",
        "\n",
        "# Find words that are only in the test list\n",
        "unique_words = words_second_list - words_first_list\n",
        "\n",
        "with open('list_of_words_test.txt', 'w') as out_file:\n",
        "    for word in unique_words:\n",
        "        out_file.write(word + '\\n')\n"
      ],
      "metadata": {
        "id": "feHj_VwK9em3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generation of datasets with LLama3-70b via TogetherAI\n",
        "\n",
        "This is the first of three phases in which LLMs play a central role in this project. In this phase, we use the LLama 3-70b model via a dedicated library provided by TogetherAI, a cloud platform for building and running generative AI. Using a one-shot prompt, LLama is tasked with generating an explanation for each word saved in the previous phase, detailing how the meaning of that word has evolved over time in the Italian language.\n",
        "\n",
        "In the dataset generation phase, we used the WiC-ITA dataset as a starting point for creating our synthetic dataset. A noteworthy aspect of the test dataset is that each term appears, on average, 10 times. We chose not to remove this redundancy for several reasons.\n",
        "\n",
        "Firstly, by prompting LLama3_70b to provide 10 different explanations for the same term, we can capture a broader range of possible meanings and nuances in its semantic evolution.\n",
        "\n",
        "Secondly, generating multiple explanations for each term helps enhance the reliability and robustness of the final explanations.\n"
      ],
      "metadata": {
        "id": "K3Xh3JjxK2WE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from together import Together\n",
        "from langchain_together import Together\n",
        "from langchain_core.prompts import PromptTemplate"
      ],
      "metadata": {
        "id": "KXYlziXaqt-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_txt(txt_path):\n",
        "    with open(txt_path, 'r', encoding='utf-8') as f:\n",
        "        content = f.read().splitlines()\n",
        "    return content\n",
        "\n",
        "# Main function to build the dataset and save the results in a file\n",
        "def build_dataset(file_parole, template_path, output_path):\n",
        "\n",
        "    list_of_words = read_txt(file_parole)\n",
        "\n",
        "    # Reads the template from the file\n",
        "    with open(template_path, 'r', encoding='utf-8') as f:\n",
        "        template = f.read()\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for word in list_of_words:\n",
        "        # Building the prompt with the current word\n",
        "        prompt_text = template.format(parola=word)\n",
        "\n",
        "        llm = Together(model=\"meta-llama/Llama-3-70b-chat-hf\", max_tokens=200, temperature=0.6, together_api_key=\"\")\n",
        "\n",
        "        response = llm(prompt_text)\n",
        "\n",
        "        explanation = response.strip()\n",
        "\n",
        "        results.append(f'{{\"lemma\": \"{word}\", \"spiegazione\": \"{explanation}\"}}')\n",
        "        #print(f'{{\"lemma\": \"{word}\", \"spiegazione\": \"{explanation}\"}}')\n",
        "\n",
        "    with open(output_path, 'w', encoding='utf-8') as f:\n",
        "        for result in results:\n",
        "            f.write(result + '\\n')\n"
      ],
      "metadata": {
        "id": "JMCWe852opKX",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_of_words = \"list_of_words_train.txt\"\n",
        "template_path = \"/content/promptLlama_con_esempio.txt\"\n",
        "output_path = \"output_llama_train.txt\"\n",
        "\n",
        "build_dataset(file_of_words, template_path, output_path)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "i-Vdc9hW36xv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_of_words = \"list_of_words_test.txt\"\n",
        "template_path = \"/content/promptLlama_con_esempio.txt\"\n",
        "output_path = \"output_llama_test.txt\"\n",
        "\n",
        "build_dataset(file_of_words, template_path, output_path)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "24NLkzgF4B1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RegExp Function to formatting the dataset\n",
        "\n",
        "The code in this section focuses on filtering and formatting the output from LLama using specific regular expressions. This process transforms the output into a jsonl dataset format, making it ready for subsequent use."
      ],
      "metadata": {
        "id": "jbWADeBvNfrE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def modify_text(text):\n",
        "    # Regex to find everything between `\"spiegazione\" :` and `La parola`, removing any characters including special ones\n",
        "    pattern_blocks = re.compile(r'(\"spiegazione\": \").*?(La parola)', re.DOTALL)\n",
        "    modified_text = pattern_blocks.sub(r'\\1La parola', text)\n",
        "\n",
        "    # Regex to remove all occurrences of 'assistant'\n",
        "    pattern_assistant = re.compile(r'\\b.assistant\\b', re.IGNORECASE)\n",
        "    modified_text = pattern_assistant.sub(\".\", modified_text)\n",
        "\n",
        "    lines = modified_text.split('\\n')\n",
        "\n",
        "    # Filter lines that do not start with a curly bracket and ensure each line ends with `\"}``\n",
        "    filtered_lines = []\n",
        "    for line in lines:\n",
        "        if line.strip().startswith(\"{\"):\n",
        "            if not line.strip().endswith('\"}'):\n",
        "                line = line.rstrip() + '\"}'\n",
        "            filtered_lines.append(line)\n",
        "\n",
        "    modified_text = '\\n'.join(filtered_lines)\n",
        "\n",
        "    # Regex to remove all quotes between 'La parola' and '\"}'\n",
        "    pattern_quotes = re.compile(r'(La parola)[^}]*?(?=\"})')\n",
        "    final_text = pattern_quotes.sub(lambda m: m.group(0).replace('\"', ''), modified_text)\n",
        "\n",
        "    return final_text\n"
      ],
      "metadata": {
        "id": "Lumd2g_N7bde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('output_llama_train.txt', 'r', encoding='utf-8') as file:\n",
        "    content = file.read()\n",
        "\n",
        "modified_content = modify_text(content)\n",
        "\n",
        "with open('terms_explanations_train.jsonl', 'w', encoding='utf-8') as file:\n",
        "    file.write(modified_content)"
      ],
      "metadata": {
        "id": "ESiP3UdR7UP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('output_llama_test.txt', 'r', encoding='utf-8') as file:\n",
        "    content = file.read()\n",
        "\n",
        "modified_content = modify_text(content)\n",
        "\n",
        "with open('terms_explanations_test.jsonl', 'w', encoding='utf-8') as file:\n",
        "    file.write(modified_content)"
      ],
      "metadata": {
        "id": "oQmQExKW7Ur2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking that json files are properly formatted\n",
        "def validate_jsonl(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for i, line in enumerate(f, start=1):\n",
        "            try:\n",
        "                json.loads(line)\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"Error in line {i}: {e}\")\n",
        "                print(line)\n",
        "\n",
        "validate_jsonl('terms_explanations_train.jsonl')\n",
        "validate_jsonl('terms_explanations_test.jsonl')\n",
        "validate_jsonl('terms_sentences_train.jsonl')\n",
        "validate_jsonl('terms_sentences_test.jsonl')"
      ],
      "metadata": {
        "id": "cQwMDY7rG3Ux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Fine tuning\n",
        "\n",
        "The fine-tuning phase is the second stage where a large language model plays a central role. In this phase, we selected Gemma2b as the baseline model for fine-tuning. This choice was primarily driven by the limited resources available. Nevertheless, the Gemma model family represents a series of lightweight, state-of-the-art open models built from the research and technology that underpinned the creation of the Gemini models. These models have demonstrated strong performance across various academic benchmarks in language understanding, reasoning, and safety."
      ],
      "metadata": {
        "id": "e-oHl5tJRLMI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install peft\n",
        "!pip install bitsandbytes\n",
        "!pip install trl"
      ],
      "metadata": {
        "id": "kr-GokVHo-uZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "test_data = []\n",
        "with open('terms_explanations_test.jsonl', 'r') as f:\n",
        "    for line in f:\n",
        "        test_data.append(json.loads(line))"
      ],
      "metadata": {
        "id": "519P1mkrROlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Test con il modello di base\n",
        "\n",
        "\n",
        "In the initial phase of fine-tuning, Gemma2b is tasked with completing the task using a zero-shot prompt. This request is made using the `generate_response` function. It is important to note that, to ensure consistency, this function will be used for subsequent models without further modifications. Analyzing Gemma2b's responses before fine-tuning establishes a baseline against which the performance of the fine-tuned models can be evaluated."
      ],
      "metadata": {
        "id": "WPGmb3DRRkRp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer"
      ],
      "metadata": {
        "id": "ZYaYngMPRR-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\")\n",
        "base_tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\")"
      ],
      "metadata": {
        "id": "HQLyLrjsRUPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `generate_response` function is designed to generate responses from a language model, focusing on penalizing repetitions to enhance the variety and quality of the outputs. Various generation parameters are configured to control the creativity and diversity of the responses:\n",
        "1. Temperature: Controls the model's creativity.\n",
        "2. Top-k: Limits the number of words the model can choose from at each step.\n",
        "3. Top-p (Nucleus Sampling): Limits the model's choices to a cumulative probability subset.\n",
        "\n",
        "Parameter values were chosen empirically, however sticking to commonly used values."
      ],
      "metadata": {
        "id": "klqGAavhgmdK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate answers with repetition penalty\n",
        "def generate_response(model, tokenizer, lemma, max_length=256):\n",
        "    input_text = f\"Descrivi brevemente come si è evoluto il significato della parola {lemma} nella lingua italiana.\"\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    # Parameters for generation\n",
        "    repetition_penalty = 1.5  # Repetition penalty Common values: [1.0, 2.0]\n",
        "    no_repeat_ngram_size = 2  # Bigramma penalty\n",
        "    temperature = 0.7  # Temperature Common values: [0.7, 1.0]\n",
        "    top_k = 50  # Top-k  Common values: 40, 50 o 100\n",
        "    top_p = 0.9  # Top-p (nucleus sampling)  Common values:  [0.8, 0.95]\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_length=max_length,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        repetition_penalty=repetition_penalty,\n",
        "        no_repeat_ngram_size=no_repeat_ngram_size,\n",
        "        temperature=temperature,\n",
        "        top_k=top_k,\n",
        "        top_p=top_p\n",
        "    )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response"
      ],
      "metadata": {
        "id": "rc5DX3nbRVsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the response of the base model\n",
        "lemma = \"accendere\"\n",
        "response = generate_response(base_model, base_tokenizer, lemma)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mw42qigeI4L-",
        "outputId": "ebe76e02-96c1-4b79-cf38-0829e2c8ab82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Descrivi brevemente come si è evoluto il significato della parola accendere nella lingua italiana.\n",
            "\n",
            "The word \"accender\" has a long history in Italian, and its meaning is constantly evolving to reflect the changing times. In ancient times, the word was used as a verb to describe lighting fires or candles, but over time it came to mean more than just igniting something: It also referred to starting up machinery or processes that required energy input from outside sources (such as electricity). Today, we use \"accende\" when referring not only these physical objects themselves – like lamps -butalso any kind of activity which requires an initial spark before proceeding further down its path towards completion!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generation of responses with the basic model\n",
        "base_responses = []\n",
        "true_explanations = []\n",
        "\n",
        "for example in test_data:\n",
        "    lemma = example[\"lemma\"]\n",
        "    true_explanation = example[\"spiegazione\"]"
      ],
      "metadata": {
        "id": "Wf8LaWQiXAJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for example in test_data:\n",
        "    base_response = generate_response(base_model, base_tokenizer, lemma)\n",
        "    base_responses.append(base_response)\n",
        "    true_explanations.append(true_explanation)"
      ],
      "metadata": {
        "id": "CFzAty_xRYgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('base_responses.json', 'w') as f:\n",
        "    json.dump(base_responses, f)"
      ],
      "metadata": {
        "id": "II8rgISzfoPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RAM memory cleaning\n",
        "del base_model\n",
        "del base_tokenizer\n",
        "torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "c3-xWIu7fm5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## First fine-tuning\n",
        "\n",
        "During this phase, the baseline model is provided with the previously generated dataset for fine-tuning to specialize in the task of lexical semantic change. The LoraConfig method is used in this phase. Low-Rank Adaptation (LoRA) is a Parameter-Efficient Fine-Tuning (PEFT) method that decomposes a large matrix into two smaller low-rank matrices in the attention layers. This significantly reduces the number of parameters that need to be fine-tuned, making the process more efficient and less resource-intensive.\n",
        "\n",
        "An essential part of this phase is defining a training prompt, which is formatted with the current lemma-explanation pair each time. The training parameters are set according to commonly used values, ensuring they are consistent with the available resources."
      ],
      "metadata": {
        "id": "kI9K-XThRhU-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments, AutoModelForCausalLM, AutoTokenizer"
      ],
      "metadata": {
        "id": "z3UUlQRyRg6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"google/gemma-2b\"\n",
        "fine_tuned_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "fine_tuned_tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "ImAiQgRqRwX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LoRA modifies the fine-tuning process by freezing the original model weights and applying changes to a separate set of weights, which are then added to the original parameters. LoRA transforms the model parameters into a lower-rank dimension, reducing the number of parameters that need training, thus speeding up the process and lowering costs."
      ],
      "metadata": {
        "id": "c2Qoq7gofwW7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "peft_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "# Adding of LoRA to the fine tuned model\n",
        "fine_tuned_model = get_peft_model(fine_tuned_model, peft_config)"
      ],
      "metadata": {
        "id": "7SgqItBORxbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = load_dataset('json', data_files='terms_explanations_train.jsonl', split='train')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "3dab9f6f5fe04b3d99fcb1c79d84fbaa",
            "b6174bfef70e464aa30d08dbb5ece52b",
            "1aa3b4fcaf004342963fcfcfab08cc05",
            "0a8e3e28d1d1434ba85a545050553d2f",
            "c6bfe63f35f042008934e5c6cdbd7c79",
            "df42d523e5134e1db275fd1edc7b8025",
            "affba0ad31af4c7f9da02fe2ed26536e",
            "b183eef5a398432494fac1635013a906",
            "c0f77e5a64e14b6684f326a2d3e89db4",
            "503023604737469ea39d99e4c3e24129",
            "61e9e124318740419f8d0f834c47dc5d"
          ]
        },
        "id": "mEA-MdLFR1Ry",
        "outputId": "cfc288a8-cb6e-4f8c-f28f-82552ec4633b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3dab9f6f5fe04b3d99fcb1c79d84fbaa"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Definition of the training prompt\n",
        "training_prompt = \"\"\"Descrivi brevemente come si è evoluto il significato della parola {} nella lingua italiana.\n",
        "Spiegazione: {}\n",
        "\"\"\"\n",
        "\n",
        "EOS_TOKEN = fine_tuned_tokenizer.eos_token\n",
        "def formatting_prompts_func(examples):\n",
        "    lemmas = examples[\"lemma\"]\n",
        "    spiegazioni = examples[\"spiegazione\"]\n",
        "    texts = []\n",
        "    for lemma, spiegazione in zip(lemmas, spiegazioni):\n",
        "        text = training_prompt.format(lemma, spiegazione) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return {\"text\": texts}"
      ],
      "metadata": {
        "id": "ocUgQcfCR7ON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = train_dataset.map(formatting_prompts_func, batched=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "82f52c6d37e34e279d31d6736ffb9912",
            "92bc8af216964e43a6b71abc17b6d0d2",
            "35f38f3a20384d0aafe09a3adef6240e",
            "92a94c986d254fd3aee54c9fa3821a97",
            "5a60bad4641a4909a746c5ea149e9d04",
            "e9ab4f0a2c9d4a5d9863ee2ac2953424",
            "687e5768f3f445a29dc4a3044ad2b20d",
            "1c8811f4711642c5a3cc405fbca5db55",
            "7518ed7295054344a7b31ac9f26131d5",
            "d11ad1d799f34437a5e6812784585981",
            "1adcc21163894567b647cde13b8dc025"
          ]
        },
        "id": "314jmgzSR-mq",
        "outputId": "23fb2f11-dd5f-4380-c745-e4b2b733460c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/2797 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "82f52c6d37e34e279d31d6736ffb9912"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(examples):\n",
        "    return fine_tuned_tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
        "\n",
        "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
        "tokenized_train_dataset = tokenized_train_dataset.rename_column(\"input_ids\", \"labels\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "6ddae0ae9ebd492c9ab6b6665ebdade5",
            "7303f70ab7664c6ab11fbb1b5d2ed7c6",
            "d5639424c4bc4ab4a44665fc879bdede",
            "b21076cfc6f64d67baed12bc1705054d",
            "6d04ab098f304ea082d8617b6cd269a7",
            "d24f5ab4bffb4ab89ee3e896128e76b5",
            "7a8e96e146e647b8beb955dcfa4e0b03",
            "60b4278b12a54c8690569b11bfb87c3b",
            "5747392fbba74580881f6f29e795a4fd",
            "cd36d636bb204c1d9afadaed8b563521",
            "829992699f9d48a1874ee651194d3db7"
          ]
        },
        "id": "qdoEXitKR7u5",
        "outputId": "ce940ae2-62e2-42f9-ac65-bfe376cf02e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/2797 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6ddae0ae9ebd492c9ab6b6665ebdade5"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training parameters\n",
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    warmup_steps=5,\n",
        "    max_steps=60,\n",
        "    learning_rate=2e-4,\n",
        "    logging_steps=1,\n",
        "    optim=\"adamw_8bit\",\n",
        "    weight_decay=0.01,\n",
        "    seed=3407,\n",
        "    output_dir=\"outputs\",\n",
        ")"
      ],
      "metadata": {
        "id": "gCNenGvMSCLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=fine_tuned_model,\n",
        "    tokenizer=fine_tuned_tokenizer,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=512,\n",
        "    dataset_num_proc=2,\n",
        "    packing=False,\n",
        "    args=training_args,\n",
        ")"
      ],
      "metadata": {
        "id": "qRFKXdIpSDaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "kyRFdxafSEiB",
        "outputId": "bb40a1b5-8fd0-4183-cb54-220e902e6dad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [60/60 04:05, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.081300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.097700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.075100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.006900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.105500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>2.048100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.963300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.938200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.835900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.907900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>1.828100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>1.753900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>1.665500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>1.580200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>1.660300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>1.529700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>1.555100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>1.425700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>1.447000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.344600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>1.291600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>1.426300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>1.353600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>1.327300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>1.313800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>1.309700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>1.302500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>1.194600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>1.287300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.197100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>1.249500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>1.306000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>1.220600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>1.115500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>1.167500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>1.201900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>1.251500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>1.150000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>1.079300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.107600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>1.197600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>1.132000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>1.116200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>1.128200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>1.221600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>1.172200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>1.123900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>1.155200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>1.215400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.153700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>1.061700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>1.084200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>1.100500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>1.064300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>1.091700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>1.147700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>1.117800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>1.079400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>1.076900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.104300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=60, training_loss=1.3874642034371694, metrics={'train_runtime': 251.6427, 'train_samples_per_second': 1.907, 'train_steps_per_second': 0.238, 'total_flos': 980188946743296.0, 'train_loss': 1.3874642034371694, 'epoch': 0.17155110793423875})"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine tuned model testing\n",
        "lemma = \"accendere\"\n",
        "response = generate_response(fine_tuned_model, fine_tuned_tokenizer, lemma)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lh7GRy8XwkT6",
        "outputId": "b5347be0-a630-48d8-fd6a-23cea1f0c212"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Descrivi brevemente come si è evoluto il significato della parola accendere nella lingua italiana.\n",
            "Spiegazione: La prima attestazione scritta di accende in italiano risale al XIV secolo, dove indicava la funzione elettrica dell'incendio o del fuoco per illuminare una stanza oppure un edificio; successivamente nel XVI e XVII secolo acquisì anche significati più astratti quali rappresentare l'atto mentale che porta alla comprensione delle idee filosofiche ed epistemologiche (accedere significa infatti comprendere). In seguito a questo uso comunemente accettato, la definizione originaria fu estesa ad includerla anche nell accezioni scientifiche relative all'elettromagnetismo e alle scienze fisici-chimico-biologiche. Infine, con lo sviluppo dei linguaggi informatici e tecnologici ha assunto ancora altri concetti relativi ai processi informativi e cognitivi basati sulla memorizzazione dati e su algoritmi computazionali. Inoltre, l’uso corrente includeva anche le definizioni correlate legate allo spargimento materiale da parte degli oggetti combustibili, così come quelle riguardanti i sistemi automatismi controllati dall'uomo tramite dispositivi elettroni\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generation of responses with the basic model\n",
        "fine_tuned_responses = []\n",
        "\n",
        "for example in test_data:\n",
        "    lemma = example[\"lemma\"]\n",
        "    true_explanation = example[\"spiegazione\"]\n",
        "\n",
        "    fine_tuned_response = generate_response(fine_tuned_model, fine_tuned_tokenizer, lemma)\n",
        "    fine_tuned_responses.append(fine_tuned_response)\n",
        "\n",
        "with open('fine_tuned_responses.json', 'w') as f:\n",
        "    json.dump(fine_tuned_responses, f)"
      ],
      "metadata": {
        "id": "9nIuzIGfSGMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean the ram\n",
        "del fine_tuned_model\n",
        "del fine_tuned_tokenizer\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "eebqJvKjPP6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Second fine-tuning\n",
        "\n",
        "Now we have entered the second fine-tuning phase, where the baseline model (Gemma) is provided with both the synthetically generated dataset and the dataset supplied by WiC. This decision was driven by the observation that both datasets focus on the same terms, as previously discussed. Consequently, we aimed to experiment and determine whether incorporating the WiC dataset would offer additional information that the model could leverage to enhance its performance. As we will demonstrate, integrating the WiC dataset into the fine-tuning process unexpectedly proved to be counterproductive."
      ],
      "metadata": {
        "id": "c-j20gJZf0gK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparation and Formatting of Datasets"
      ],
      "metadata": {
        "id": "hbTnmKdREh_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from datasets import load_dataset, Dataset"
      ],
      "metadata": {
        "id": "cKEORdLwPSLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_train_data = []\n",
        "with open('terms_sentences_train.jsonl', 'r') as f:\n",
        "    for line in f:\n",
        "        new_train_data.append(json.loads(line))"
      ],
      "metadata": {
        "id": "z_D0DqcLBSzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Formatting the new dataset\n",
        "def format_new_dataset(data):\n",
        "    formatted_data = []\n",
        "    for item in data:\n",
        "        lemma = item[\"lemma\"]\n",
        "        sentence1 = item[\"sentence1\"]\n",
        "        sentence2 = item[\"sentence2\"]\n",
        "        label = item[\"label\"]\n",
        "        instruction = \"Determina se la parola ha lo stesso significato in entrambe le frasi.\"\n",
        "        input_text = f\"Parola: {lemma}\\nFrase 1: {sentence1}\\nFrase 2: {sentence2}\"\n",
        "        response_text = f\"Label: {label}\"\n",
        "        formatted_data.append({\n",
        "            \"instruction\": instruction,\n",
        "            \"input\": input_text,\n",
        "            \"output\": response_text\n",
        "        })\n",
        "    return formatted_data\n",
        "\n",
        "formatted_new_train_data = format_new_dataset(new_train_data)"
      ],
      "metadata": {
        "id": "zL15HPWCBsaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading existing train dataset\n",
        "train_data = []\n",
        "with open('terms_explanations_train.jsonl', 'r') as f:\n",
        "    for line in f:\n",
        "        train_data.append(json.loads(line))"
      ],
      "metadata": {
        "id": "h2eH44zuEPcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Formatting for existing train dataset.\n",
        "def format_existing_dataset(data):\n",
        "    formatted_data = []\n",
        "    for item in data:\n",
        "        lemma = item[\"lemma\"]\n",
        "        spiegazione = item[\"spiegazione\"]\n",
        "        instruction = \"Descrivi come si è evoluto il significato della parola data nella lingua italiana.\"\n",
        "        input_text = f\"Lemma: {lemma}\"\n",
        "        response_text = f\"Spiegazione: {spiegazione}\"\n",
        "        formatted_data.append({\n",
        "            \"instruction\": instruction,\n",
        "            \"input\": input_text,\n",
        "            \"output\": response_text\n",
        "        })\n",
        "    return formatted_data\n",
        "\n",
        "formatted_train_data = format_existing_dataset(train_data)"
      ],
      "metadata": {
        "id": "9YpDanGqEGzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Union of the two datasets and conversion to Dataset\n",
        "format.combined_train_data = formatted_train_data + formatted_new_train_data\n",
        "\n",
        "train_dataset = Dataset.from_list(combined_train_data)"
      ],
      "metadata": {
        "id": "GthAdE3Q9TBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine-Tuning\n",
        "\n",
        "To maintain consistency in the fine-tuning process for both models, the same training parameters and training prompts were used.\n",
        "This decision was made to ensure that any differences in the performance of the models could be attributed to the variations in the datasets rather than changes in the training configuration."
      ],
      "metadata": {
        "id": "nHaIf4vkEk1g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments, AutoModelForCausalLM, AutoTokenizer"
      ],
      "metadata": {
        "id": "W_u_ZPknE6-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"google/gemma-2b\"\n",
        "fine_tuned_model_2 = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "fine_tuned_tokenizer_2 = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120,
          "referenced_widgets": [
            "0c92fdcdaee7489397f59e116c2443ab",
            "53e183a626ca4aeabc2bd275ee45748a",
            "0a3f7f12390341d195baf76cf6bf5d06",
            "91f56f5726db44ffb19039e8e7da9ed6",
            "ee78faea95e04c31886d366c3427d39a",
            "ace044b556ff4d80b65f48a339937b28",
            "9e5e880a3a524507a14967fec2795549",
            "e179147b970f4cb9a1e466b94fc65897",
            "d76a78cdf4c34771b72dfe29942024e0",
            "cc6977621129492bb4b0b95f94e495d7",
            "88aa7ef4b9df4a4299681017560f2519"
          ]
        },
        "id": "xQA3Ds4_FUjA",
        "outputId": "699f0314-82ae-4047-db58-f6a47faff79c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
            "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
            "`config.hidden_activation` if you want to override this behaviour.\n",
            "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0c92fdcdaee7489397f59e116c2443ab"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "peft_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "fine_tuned_model_2 = get_peft_model(fine_tuned_model_2, peft_config)"
      ],
      "metadata": {
        "id": "1anXCTXBFWkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_prompt = \"\"\"Lemma: {}\n",
        "Spiegazione: {}\n",
        "\"\"\"\n",
        "\n",
        "EOS_TOKEN = fine_tuned_tokenizer_2.eos_token\n",
        "def formatting_prompts_func(examples):\n",
        "    inputs = examples[\"input\"]\n",
        "    outputs = examples[\"output\"]\n",
        "    texts = []\n",
        "    for input_text, output in zip(inputs, outputs):\n",
        "        text = training_prompt.format(input_text, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return {\"text\": texts}"
      ],
      "metadata": {
        "id": "OFrZHMM3FcrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = train_dataset.map(formatting_prompts_func, batched=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "35bb7332ab7f476ea5db7a29c437c307",
            "5c9b013686dd435084bbbb958bed45fc",
            "6b846ad868fd496c97e392726cfeb8d6",
            "8bf37d29c3444a6c9d7491f7f630da56",
            "01e1f6d58b584ac19169b60707012b80",
            "8012a9653e83459793a3d237434e0bba",
            "f98463c9c56a446689eade305ea24c32",
            "95e51857038f4000a9270fc66fb24dac",
            "967f6508e2eb4649bde0c79e10b68034",
            "cea6e832b0ce413383a11bf3cc2413b8",
            "cb49ff51997b48528f8fa15b4d1c1c22"
          ]
        },
        "id": "fzXgV4AmFfv_",
        "outputId": "ae752d2a-768f-4913-9284-b7bdc72a4f36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/5602 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "35bb7332ab7f476ea5db7a29c437c307"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(examples):\n",
        "    return fine_tuned_tokenizer_2(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
        "\n",
        "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
        "tokenized_train_dataset = tokenized_train_dataset.rename_column(\"input_ids\", \"labels\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "518c7c01bf1c45e8ab83abc06a060ab2",
            "28e86a00bf4c4199b8a4cf251ef91e32",
            "196529ae977d4c5c9c027ce16e44fbcb",
            "d69c6d50ea4f481ab0dbc1e01d3036bd",
            "bbfa207b36524dcdb886534a0f42b88d",
            "4574c31aa8a844d5bbc4d8c71eb9180c",
            "16c06a1144e14d2089e22b7c4ebfa7aa",
            "943e5efa79fc4b989508311c48e4a5d1",
            "de95f3cb5f05446d841fe6034d41ea58",
            "45ba5988a8664444870e3eb4467c0416",
            "04dd3c3aae0d4d0aab91bd59676b00d2"
          ]
        },
        "id": "AeNQLp6mFk7i",
        "outputId": "9b38d121-74ee-43c6-a352-0f3974e6de7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/5602 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "518c7c01bf1c45e8ab83abc06a060ab2"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    warmup_steps=5,\n",
        "    max_steps=60,\n",
        "    learning_rate=2e-4,\n",
        "    logging_steps=1,\n",
        "    optim=\"adamw_8bit\",\n",
        "    weight_decay=0.01,\n",
        "    seed=3407,\n",
        "    output_dir=\"outputs\",\n",
        ")"
      ],
      "metadata": {
        "id": "9uG2EAUyFmcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=fine_tuned_model_2,\n",
        "    tokenizer=fine_tuned_tokenizer_2,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=512,\n",
        "    dataset_num_proc=2,\n",
        "    packing=False,\n",
        "    args=training_args,\n",
        ")"
      ],
      "metadata": {
        "id": "3Od7vJlzFnp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "w8HsyqDKEagh",
        "outputId": "d60af28f-996d-4ed5-dc55-252c8d16dca8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [60/60 03:33, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.086400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.050900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.908400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>3.550300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.191200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>3.486400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>2.331600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>2.234100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>2.605700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>3.407000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>3.369100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>2.399500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>3.010300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>2.413700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>2.850500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>2.236700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>2.140600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>2.591100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>2.139900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>2.781200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>2.873600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>2.094200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>2.245900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>2.165600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>2.759200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>2.360700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>2.355600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>1.922900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>2.354900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.915400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>2.337500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>2.179600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>2.198800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>2.292200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>2.427000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>2.384600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>2.123400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>1.981400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>1.793300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.869100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>2.199300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>2.262200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>2.406900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>1.376300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>2.132500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>2.011500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>2.103300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>2.016300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>2.382800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.189300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>1.990400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>2.474500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>2.450800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>1.839900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>1.886800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>1.940700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>1.933700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>1.688800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>2.153400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>2.226100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=60, training_loss=2.351414555311203, metrics={'train_runtime': 219.1774, 'train_samples_per_second': 2.19, 'train_steps_per_second': 0.274, 'total_flos': 845424284319744.0, 'train_loss': 2.351414555311203, 'epoch': 0.08568368439842913})"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the second fine tuned model's response\n",
        "lemma = \"accendere\"\n",
        "response = generate_response(fine_tuned_model_2, fine_tuned_tokenizer_2, lemma)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClAoCNqWN-k5",
        "outputId": "e7981332-efd5-4c85-8109-5216c1816b0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Descrivi brevemente come si è evoluto il significato della parola accendere nella lingua italiana.\n",
            "Come ha cambiato la definizione di acchiappare nel corso del tempo?\n",
            "\n",
            "Grazie!\n",
            "Ciao,\n",
            "la prima domanda non mi sembra molto chiara: cosa intendi per \"accendersi\"? Se ti riferisci alla luce che viene fuori quando un oggetto o una persona entra in contatto con l'aria (ad esempio se qualcuno spara a te), allora sì, questa accezione era già presente all’inizio dell’XI secolo; ma poi questo senso scomparve e fu ripreso solo verso fine XVI-inizi XVII secolo da Giovanni Battista Guarini nell’opera La grammatica moderna .\n",
            "La seconda invece chiedeva semplicemente quale fosse stato lo sviluppo storico delle parole 'accendire', 'sputare' , 'spegnere'. In realtà queste due ultime erano state introdotte dal latino medievale : ad es., le prime attestate sono quelle riportata dalla Enciclopedia Treccani mentre quella relativa al secondo termine proviene dall’Enciclopedia Italiana 1930 ; quest’ultima però indica anche altre espressioni latine quali speglare , far giuare , fare giuro , eccetera .\n",
            "\n",
            "<blockquote>gugo82 ha scritto: Ciao\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating responsens with second fine tuned model\n",
        "fine_tuned_responses = []\n",
        "\n",
        "for example in test_data:\n",
        "    lemma = example[\"lemma\"]\n",
        "    true_explanation = example[\"spiegazione\"]\n",
        "\n",
        "    fine_tuned_response = generate_response(fine_tuned_model_2, fine_tuned_tokenizer_2, lemma)\n",
        "    fine_tuned_responses.append(fine_tuned_response)\n",
        "\n",
        "with open('fine_tuned_responses_2.json', 'w') as f:\n",
        "    json.dump(fine_tuned_responses, f)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dkcGOL2AFq0p",
        "outputId": "1c19f5e8-c774-444e-920e-e0eade3ad327"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del fine_tuned_model_2\n",
        "del fine_tuned_tokenizer_2\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "7ngSfutoWG3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Evaluation\n",
        "\n",
        "In the evaluation phase, the objective was to compare the two fine-tuned models with the baseline model. This evaluation was conducted in two main instances.\n",
        "\n",
        "Firstly, state-of-the-art metrics such as BERTScore, BLEU, and ROUGE were used. These metrics provide a quantitative assessment of the models' performance in generating accurate and relevant explanations for lexical semantic change.\n",
        "\n",
        "Secondly, we experimented with using an LLM, specifically LLama3_70b, as a qualitative evaluator of the responses provided by the two fine-tuned models. This approach aimed to leverage the advanced language understanding capabilities of LLama3_70b to provide a more nuanced assessment of the generated explanations, beyond what can be captured by quantitative metrics alone.\n",
        "\n",
        "By combining both quantitative and qualitative evaluations, we aimed to gain a comprehensive understanding of the strengths and limitations of the fine-tuned models compared to the baseline, ensuring a robust assessment of their performance."
      ],
      "metadata": {
        "id": "nFz1NjPze0S0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Quantitative Evaluation\n",
        "\n",
        "We consider BERTScore as the primary metric for evaluating the models in this task. Its ability to capture the semantic meaning of sentences makes it particularly suitable for assessing explanations of lexical semantic change. Since understanding and explaining the evolution of word meanings require a deep grasp of context and semantics, BERTScore’s approach ensures that the generated explanations are not only lexically accurate but also semantically coherent and meaningful. This makes it the most crucial metric for our evaluation.\n",
        "\n",
        "While BERTScore is our primary metric, BLEU provides additional insights into the accuracy of word and phrase generation. It helps assess whether the generated explanations contain the correct terms and expressions found in the reference explanations. BLEU is included to observe whether the fine-tuned models can produce text that closely mirrors the reference dataset in terms of exact word usage.\n",
        "\n",
        "ROUGE is particularly useful for evaluating the comprehensiveness of the generated explanations. Given that our task involves generating detailed and accurate historical explanations of word meanings, ROUGE helps ensure that the generated text captures the necessary breadth of information. By focusing on recall, ROUGE metrics confirm that the fine-tuned models do not miss critical elements present in the reference explanations.\n",
        "\n",
        "#### Combined Rationale:\n",
        "While BERTScore serves as the cornerstone of our evaluation due to its superior semantic evaluation capabilities, BLEU and ROUGE are utilized for a more comprehensive analysis. BLEU analyzes precision and exact word matching, and ROUGE assesses recall and completeness. This multi-faceted approach provides a well-rounded assessment, capturing both the lexical and semantic quality of the generated explanations.\n",
        "\n",
        "Notice that we did not apply BLEU and ROUGE to the baseline model, as it generates responses in English, which would render these metrics ineffective due to their sensitivity to language-specific structures. Translating the baseline responses into Italian for these metrics would introduce an additional variable in the evaluation, potentially skewing the results."
      ],
      "metadata": {
        "id": "3Iick-L7GHyT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bert_score rouge_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "WRvvPWdsVtbU",
        "outputId": "9eaafc5b-482e-4e54-c224-f5eadce6310d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bert_score\n",
            "  Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/61.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.3.0+cu121)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.0.3)\n",
            "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from bert_score) (4.41.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bert_score) (1.25.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.10/dist-packages (from bert_score) (4.66.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from bert_score) (3.7.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from bert_score) (24.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.8.1)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2024.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.0.0->bert_score)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.0.0->bert_score)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.0.0->bert_score)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.0.0->bert_score)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.0.0->bert_score)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.0.0->bert_score)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.0.0->bert_score)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.0.0->bert_score)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.0.0->bert_score)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.0.0->bert_score)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.0.0->bert_score)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.0.0->bert_score)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.23.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.4.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (3.1.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.4.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->bert_score) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.0.0->bert_score) (1.3.0)\n",
            "Building wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24933 sha256=b49c2278f952f3e0486b7ab28df85e5de6b6b60a62568fb73872d7c48d953849\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, rouge_score, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bert_score\n",
            "Successfully installed bert_score-0.3.13 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 rouge_score-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from bert_score import score\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from rouge_score import rouge_scorer\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "jhWF_IlJTrB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('base_responses.json', 'r') as f:\n",
        "    base_responses = json.load(f)\n",
        "\n",
        "with open('fine_tuned_responses.json', 'r') as f:\n",
        "    fine_tuned_responses = json.load(f)\n",
        "\n",
        "with open('fine_tuned_responses_2.json', 'r') as f:\n",
        "    fine_tuned_responses_2 = json.load(f)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "eVXE73eVFFa4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Uploading reference explanations\n",
        "with open('terms_explanations_test.jsonl', 'r') as f:\n",
        "    true_explanations = [json.loads(line)[\"spiegazione\"] for line in f]\n",
        "\n",
        "# Ensure that the number of responses and explanations is the same\n",
        "assert len(base_responses) == len(true_explanations), \"Numero di risposte del modello di base diverso dal numero di spiegazioni\"\n",
        "assert len(fine_tuned_responses) == len(true_explanations), \"Numero di risposte del primo modello fine-tunato diverso dal numero di spiegazioni\"\n",
        "assert len(fine_tuned_responses_2) == len(true_explanations), \"Numero di risposte del secondo modello fine-tunato diverso dal numero di spiegazioni\""
      ],
      "metadata": {
        "id": "ehyoi7nAZNFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BERTScore calculation for the base model\n",
        "P_base, R_base, F1_base = score(base_responses, true_explanations, lang=\"it\", verbose=True)\n",
        "\n",
        "# BERTScore calculation for the first fine tuned model\n",
        "P_fine, R_fine, F1_fine = score(fine_tuned_responses, true_explanations, lang=\"it\", verbose=True)\n",
        "\n",
        "# BERTScore calculation for the first fine tuned model\n",
        "P_fine_2, R_fine_2, F1_fine_2 = score(fine_tuned_responses_2, true_explanations, lang=\"it\", verbose=True)"
      ],
      "metadata": {
        "id": "1JKfu7RaXwrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BLEU\n",
        "reference_list = [[ref.split()] for ref in true_explanations]\n",
        "fine_list = [resp.split() for resp in fine_tuned_responses]\n",
        "fine_2_list = [resp.split() for resp in fine_tuned_responses_2]\n",
        "\n",
        "bleu_fine = corpus_bleu(reference_list, fine_list)\n",
        "bleu_fine_2 = corpus_bleu(reference_list, fine_2_list)"
      ],
      "metadata": {
        "id": "2GveuJm1ULKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ROUGE\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "rouge_fine_scores = [scorer.score(ref, resp) for ref, resp in zip(true_explanations, fine_tuned_responses)]\n",
        "rouge_fine_2_scores = [scorer.score(ref, resp) for ref, resp in zip(true_explanations, fine_tuned_responses_2)]\n",
        "\n",
        "def avg_rouge_scores(rouge_scores):\n",
        "    rouge1 = {\"precision\": 0, \"recall\": 0, \"fmeasure\": 0}\n",
        "    rouge2 = {\"precision\": 0, \"recall\": 0, \"fmeasure\": 0}\n",
        "    rougeL = {\"precision\": 0, \"recall\": 0, \"fmeasure\": 0}\n",
        "\n",
        "    for score in rouge_scores:\n",
        "        rouge1[\"precision\"] += score[\"rouge1\"].precision\n",
        "        rouge1[\"recall\"] += score[\"rouge1\"].recall\n",
        "        rouge1[\"fmeasure\"] += score[\"rouge1\"].fmeasure\n",
        "        rouge2[\"precision\"] += score[\"rouge2\"].precision\n",
        "        rouge2[\"recall\"] += score[\"rouge2\"].recall\n",
        "        rouge2[\"fmeasure\"] += score[\"rouge2\"].fmeasure\n",
        "        rougeL[\"precision\"] += score[\"rougeL\"].precision\n",
        "        rougeL[\"recall\"] += score[\"rougeL\"].recall\n",
        "        rougeL[\"fmeasure\"] += score[\"rougeL\"].fmeasure\n",
        "\n",
        "    num_scores = len(rouge_scores)\n",
        "    for key in rouge1:\n",
        "        rouge1[key] /= num_scores\n",
        "        rouge2[key] /= num_scores\n",
        "        rougeL[key] /= num_scores\n",
        "\n",
        "    return {\"rouge1\": rouge1, \"rouge2\": rouge2, \"rougeL\": rougeL}\n",
        "\n",
        "avg_rouge_fine = avg_rouge_scores(rouge_fine_scores)\n",
        "avg_rouge_fine_2 = avg_rouge_scores(rouge_fine_2_scores)"
      ],
      "metadata": {
        "id": "oQckAQkOUNg3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creazione del DataFrame originale con i dati calcolati\n",
        "results = {\n",
        "    \"Metric\": [\"BERTScore Precision\", \"BERTScore Recall\", \"BERTScore F1\",\n",
        "               \"BLEU\",\n",
        "               \"ROUGE-1 Precision\", \"ROUGE-1 Recall\", \"ROUGE-1 F1\",\n",
        "               \"ROUGE-2 Precision\", \"ROUGE-2 Recall\", \"ROUGE-2 F1\",\n",
        "               \"ROUGE-L Precision\", \"ROUGE-L Recall\", \"ROUGE-L F1\"],\n",
        "    \"Base Model\": [P_base.mean().item(), R_base.mean().item(), F1_base.mean().item(),\n",
        "                   None,\n",
        "                   None, None, None,\n",
        "                   None, None, None,\n",
        "                   None, None, None],\n",
        "    \"Fine-tuned Model 1\": [P_fine.mean().item(), R_fine.mean().item(), F1_fine.mean().item(),\n",
        "                           bleu_fine,\n",
        "                           avg_rouge_fine[\"rouge1\"][\"precision\"], avg_rouge_fine[\"rouge1\"][\"recall\"], avg_rouge_fine[\"rouge1\"][\"fmeasure\"],\n",
        "                           avg_rouge_fine[\"rouge2\"][\"precision\"], avg_rouge_fine[\"rouge2\"][\"recall\"], avg_rouge_fine[\"rouge2\"][\"fmeasure\"],\n",
        "                           avg_rouge_fine[\"rougeL\"][\"precision\"], avg_rouge_fine[\"rougeL\"][\"recall\"], avg_rouge_fine[\"rougeL\"][\"fmeasure\"]],\n",
        "    \"Fine-tuned Model 2\": [P_fine_2.mean().item(), R_fine_2.mean().item(), F1_fine_2.mean().item(),\n",
        "                           bleu_fine_2,\n",
        "                           avg_rouge_fine_2[\"rouge1\"][\"precision\"], avg_rouge_fine_2[\"rouge1\"][\"recall\"], avg_rouge_fine_2[\"rouge1\"][\"fmeasure\"],\n",
        "                           avg_rouge_fine_2[\"rouge2\"][\"precision\"], avg_rouge_fine_2[\"rouge2\"][\"recall\"], avg_rouge_fine_2[\"rouge2\"][\"fmeasure\"],\n",
        "                           avg_rouge_fine_2[\"rougeL\"][\"precision\"], avg_rouge_fine_2[\"rougeL\"][\"recall\"], avg_rouge_fine_2[\"rougeL\"][\"fmeasure\"]],\n",
        "}\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Creazione della tabella per BERTScore\n",
        "bertscore_df = results_df[results_df['Metric'].str.contains('BERTScore')]\n",
        "\n",
        "# Creazione della tabella per BLEU e ROUGE (escludendo il modello di base)\n",
        "bleu_rouge_df = results_df[results_df['Metric'].str.contains('BLEU|ROUGE')].drop(columns=['Base Model'])\n",
        "\n",
        "# Stampa delle tabelle\n",
        "print(\"BERTScore Comparison:\")\n",
        "print(bertscore_df)\n",
        "\n",
        "print(\"\\nBLEU and ROUGE Comparison (Fine-tuned Models Only):\")\n",
        "print(bleu_rouge_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMkQYuOEUzHG",
        "outputId": "eea1e834-ed7c-4695-9c6a-93d568c037eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERTScore Comparison:\n",
            "                Metric  Base Model  Fine-tuned Model 1  Fine-tuned Model 2\n",
            "0  BERTScore Precision    0.634947            0.692616            0.653618\n",
            "1     BERTScore Recall    0.666521            0.738854            0.681913\n",
            "2         BERTScore F1    0.650136            0.714930            0.667063\n",
            "\n",
            "BLEU and ROUGE Comparison (Fine-tuned Models Only):\n",
            "               Metric  Fine-tuned Model 1  Fine-tuned Model 2\n",
            "3                BLEU            0.019053            0.015501\n",
            "4   ROUGE-1 Precision            0.247815            0.253092\n",
            "5      ROUGE-1 Recall            0.380555            0.290233\n",
            "6          ROUGE-1 F1            0.298872            0.254315\n",
            "7   ROUGE-2 Precision            0.047776            0.048094\n",
            "8      ROUGE-2 Recall            0.072917            0.049930\n",
            "9          ROUGE-2 F1            0.057449            0.045095\n",
            "10  ROUGE-L Precision            0.117114            0.123879\n",
            "11     ROUGE-L Recall            0.179274            0.136418\n",
            "12         ROUGE-L F1            0.141075            0.121223\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation Results\n",
        "\n",
        "From the evaluation using the metrics, it is evident that the first fine-tuned model, which utilizes only the synthetically generated dataset, exhibits the best performance. Specifically, it achieved a BERTScore F1 of 0.71. The second model, fine-tuned on both the generated dataset and the WiC dataset, also performed better than the baseline but showed significantly degraded performance compared to the first model.\n"
      ],
      "metadata": {
        "id": "Ef-SWdRI5jXE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Qualitative Evaluation\n",
        "\n",
        "The evaluation phase with LLama3_70b represents the third stage where an LLM takes center stage in our project. In this phase, LLama3_70b is used as a qualitative evaluator. By providing it with an appropriate prompt, we ask the model to indicate which of the responses given is the most accurate. This approach stems from the idea that BERTScore is essentially a comparison with BERT embeddings, which assess the semantic similarity between generated and reference texts. Similarly, using LLama3_70b for evaluation is akin to comparing the responses against LLama3_70b embeddings, leveraging its advanced language understanding capabilities.\n",
        "\n",
        "To conduct this evaluation, we performed five tests where the prompt included responses provided by the baseline model and the two fine-tuned models for the same word. The responses were selected randomly to ensure unbiased evaluation. LLama3_70b was then asked to determine which response was the most accurate.\n",
        "\n",
        "As we can see form the results, this qualitative assessment also indicated that the first fine-tuned model, which used only the synthetically generated dataset, was superior. LLama3_70b consistently identified responses from this model as the most accurate compared to those from the baseline model and the second fine-tuned model."
      ],
      "metadata": {
        "id": "4vN0cHmBfz51"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import json\n",
        "import requests"
      ],
      "metadata": {
        "id": "FwdTifj2wqDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('base_responses.json', 'r') as f:\n",
        "    base_responses = json.load(f)\n",
        "\n",
        "with open('fine_tuned_responses.json', 'r') as f:\n",
        "    fine_tuned_responses = json.load(f)\n",
        "\n",
        "with open('fine_tuned_responses_2.json', 'r') as f:\n",
        "    fine_tuned_responses_2 = json.load(f)"
      ],
      "metadata": {
        "id": "5ktQxMhXwt7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading reference explanations\n",
        "true_explanations = []\n",
        "true_terms = []\n",
        "with open('terms_explanations_test.jsonl', 'r') as f:\n",
        "    for line in f:\n",
        "        data = json.loads(line)\n",
        "        true_explanations.append(data[\"spiegazione\"])\n",
        "        true_terms.append(data[\"lemma\"])"
      ],
      "metadata": {
        "id": "vNtWh43M2lon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert len(base_responses) == len(fine_tuned_responses) == len(fine_tuned_responses_2) == len(true_explanations) == len(true_terms), \"Le lunghezze dei dataset non corrispondono.\""
      ],
      "metadata": {
        "id": "bu0JLswy2oBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select 5 random answers\n",
        "indices = random.sample(range(len(true_explanations)), 5)\n",
        "\n",
        "selected_base_responses = [base_responses[i] for i in indices]\n",
        "selected_fine_tuned_responses = [fine_tuned_responses[i] for i in indices]\n",
        "selected_fine_tuned_responses_2 = [fine_tuned_responses_2[i] for i in indices]\n",
        "selected_true_explanations = [true_explanations[i] for i in indices]\n",
        "selected_true_terms = [true_terms[i] for i in indices]"
      ],
      "metadata": {
        "id": "rU4G6Vi1wwiN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_together"
      ],
      "metadata": {
        "id": "qlwBm5sFxMy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from langchain_together import Together\n",
        "from langchain_core.prompts import PromptTemplate"
      ],
      "metadata": {
        "id": "PWXe-CjbQU2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#api_key = \"\"\n",
        "\n",
        "responses = []\n",
        "\n",
        "for i in range(5):\n",
        "    prompt = f\"\"\"\n",
        "    Istruzioni: Di seguito sono riportate tre risposte alla stessa richiesta. Valuta quale risposta è la più accurata in base al contesto fornito.\n",
        "\n",
        "    Contesto: Descrivi come si è evoluto il significato della parola {selected_true_terms[i]} nella lingua italiana.\n",
        "\n",
        "    Risposta 1: {selected_base_responses[i]}\n",
        "\n",
        "    Risposta 2: {selected_fine_tuned_responses[i]}\n",
        "\n",
        "    Risposta 3: {selected_fine_tuned_responses_2[i]}\n",
        "\n",
        "    Il tuo compito è scegliere la risposta più accurata e motivare la scelta.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"Risposta 1 (modello base): \\n\", selected_base_responses[i])\n",
        "    print(\"-----------------------------------------------\\n\")\n",
        "    print(\"Risposta 2 (primo modello fine-tunato): \\n\", selected_fine_tuned_responses[i])\n",
        "    print(\"-----------------------------------------------\\n\")\n",
        "    print(\"Risposta 3 (secondo modello fine-tunato): \\n\", selected_fine_tuned_responses_2[i])\n",
        "    print(\"-----------------------------------------------\\n\")\n",
        "    model = Together(model=\"meta-llama/Llama-3-70b-chat-hf\", max_tokens=180, temperature=0.4, together_api_key=\"\")\n",
        "    prompt = PromptTemplate.from_template(prompt)\n",
        "    chain = prompt | model\n",
        "    response = chain.invoke({\"selected_true_terms\": selected_true_terms[i], \"selected_base_responses\":selected_base_responses[i], \"selected_fine_tuned_responses\":selected_fine_tuned_responses[i], \"selected_fine_tuned_responses_2\":selected_fine_tuned_responses_2[i]})\n",
        "    print(response)\n",
        "    print(\"***********************************************\\n\")\n",
        "    print(\"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fCjHt2DwkgO",
        "outputId": "eca08229-33e1-4d63-db0c-d54b77e2ba55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Risposta 1 (modello base): \n",
            " Descrivi brevemente come si è evoluto il significato della parola minore nella lingua italiana.\n",
            "\n",
            "The word \"minor\" has been used in Italian since the 16th century, but its meaning and usage have changed over time. In the early 20th Century, it was commonly used to refer to a person who is not of noble birth or status, such as a servant or a tradesman. However, by the mid-century, the term had taken on a more specific meaning related to music, specifically referring to an instrument that plays lower notes than the standard orchestra. Today, minor can also be used metaphorically to describe something small or insignificant compared to other things.\n",
            "-----------------------------------------------\n",
            "\n",
            "Risposta 2 (primo modello fine-tunato): \n",
            " Descrivi brevemente come si è evoluto il significato della parola minore nella lingua italiana.\n",
            "Spiegazione: La prima attestazione scritta di minori in italiano risale al XII secolo, dove indicava un gruppo o una categoria inferiore rispetto ad altre entità simili (ad esempio la classe sociale dei contadini). Nel Medioevo e nel Rinascimento questo termine era utilizzato anche per descrivere persone che erano state esiliate da uno stato oppure fuggite dalla loro patria; successivamente fu usato più ampiamente nell'ambito religioso ed ecclesiastico per riferirsi a sacerdoti ordinati inferiormente ai vescovi. In seguito alla Rivoluzione francese del 1789, l'espressione divenne comune nei paesi europeei occidentale per designare le classi sociali meno abbienti dell'intera popolazione - tra cui i poveri comuni, gli operai industriali e lo slavo-italiano. Successivamente ha assunto significati diversi nelle scienze socialismidrule, ovvero gruppi minoritari con caratteristiche diverse dalle restanti componenti dello stesso sistema sociologico. Infine, la terminologia viene utilizzata frequentemente in campo politico per definire partiti politici o movimenti popolari avente tendenze democratiche e liberiste. Inoltre, minori può essere usata anche come sinon\n",
            "-----------------------------------------------\n",
            "\n",
            "Risposta 3 (secondo modello fine-tunato): \n",
            " Descrivi brevemente come si è evoluto il significato della parola minore nella lingua italiana.\n",
            "Come ha cambiato la definizione di minoranza? Come hanno mutata le idee e i valori che accompagnavano questa categoria sociale nel corso del tempo ?\n",
            "Describe briefly how the meaning of the word 'minor' has evolved in the Italian language. How have ideas and values associated with this social category changed over time?\n",
            "In quale periodo storico l’idea dell ‘uomo libero ’ assume un nuovo valore, oltre a quello politico-sociale , assumendo anche una dimensione religiosa ed estetica .\n",
            "What period is characterized by an idea that goes beyond political - social value to include religious and aesthetic dimensions as well?\n",
            "-----------------------------------------------\n",
            "\n",
            "assistant\n",
            "\n",
            "La risposta più accurata è la seconda.\n",
            "\n",
            "La motivazione è la seguente:\n",
            "\n",
            "La seconda risposta fornisce una descrizione più dettagliata e precisa dell'evoluzione del significato della parola \"minore\" nella lingua italiana. La risposta copre un arco di tempo più ampio, dal XII secolo fino ai giorni nostri, e fornisce esempi concreti di come il termine sia stato utilizzato in diversi contesti, come la religione, la politica e la sociologia. Inoltre, la risposta fornisce una visione più ampia del cambiamento del significato della parola \"minore\" nel corso del tempo, mostrando come sia stato influenzato da eventi storici come la Rivoluzione francese.\n",
            "\n",
            "\n",
            "***********************************************\n",
            "\n",
            "\n",
            "\n",
            "Risposta 1 (modello base): \n",
            " Descrivi brevemente come si è evoluto il significato della parola formazione nella lingua italiana.\n",
            "\n",
            "The word formation is a very important concept in linguistics, and it has been the subject of many studies over time (e.g., Chomsky 1965; Halliday 203). In this paper I will try to describe how its meaning evolved from one period into another by looking at some examples taken mainly from Italian language literature but also including English ones for comparison purposes: first-person singular pronouns “I” and “me” are considered as paradigmatic cases because they have undergone several changes during their history that can be traced back through different periods or stages within which linguistic change occurs more rapidly than others due either naturally occurring processes such as mutation/mutationism vs replacement etc.. The aim here isn’t just about describing these phenomena linguistically though – rather we want them understood better so people know what kind off words exist today compared with those used centuries ago when there wasn’ t much difference between languages like Latin & Greek .\n",
            "-----------------------------------------------\n",
            "\n",
            "Risposta 2 (primo modello fine-tunato): \n",
            " Descrivi brevemente come si è evoluto il significato della parola formazione nella lingua italiana.\n",
            "Spiegazione: La prima attestazione di formazzione in italiano risale al XII secolo, dove indicava la pratica o l'attività che consisteva nel creare forme letterarie e artistiche per un testo scritto; successivamente, a partire dal XVI-XVII secolo fu utilizzata anche con riferimento alla creazione delle opere d’arte (come ad esempio le pitture). Nel XVIII - XIX secolo acquisì una nuova acce~sione relativa all’istruzione formativa ed educativa, indicando lo studio dei contenuti teorici nelle discipline scientifiche e umanistiche. In seguito, durante Il XX secolo ha assunto significati più generali quali quello relativo alle attività sociali e culturali legate ai processi educativi e ricreative degli individui. Infine, nell'Ottocento, la terminologia scomparve dalla usage comune ma venne riportata da alcuni linguisti italiani negli anni '50 del Novecento per descrivere i metodi pedagogistici basati sulla teoria dell'educazione psicopedagogica fondato su principi psicologiche e socioculturali. Spesso utilizzato anche nei campi dello sport e della cultura popolare, tale termine indica oggi uno stato mentale positivo caratterizzato dall'autocont\n",
            "-----------------------------------------------\n",
            "\n",
            "Risposta 3 (secondo modello fine-tunato): \n",
            " Descrivi brevemente come si è evoluto il significato della parola formazione nella lingua italiana.\n",
            "Come ha cambiato la definizione di forma nel corso del tempo?\n",
            "\n",
            "Inizialmente, la voce era utilizzata per descrivere un'area o una regione geografica: ad esempio \"la zona in cui sono state formate le città\" oppure \"il territorio che comprende l'Italia\". Con lo sviluppo dell'uso comune e delle tecniche scientifiche , però, questo senso iniziale iniziò a cambiare . In primo luogo, con i progressi tecnologici fu introdotta anche la nozione più generale di creazione : così veniva utilizzato per indicare processi quali la nascita (come quando viene formata una persona) o la crescita fisica (ad es., la maturazione dei capelli). Inoltre, durante gli anni '60-70 , la terminologia venne ampliata includendo nuovi concetti relativi alla cultura ed al modo d' essere umano - tra questi vi erano quelli di identità individuale e sociale nonché quello di comunità locale o nazionale . Infine , negli ultimi decenni , questa idea originaria scomparve completamente dal linguaggio italiano sostituita da altre parole specifiche come struttura organizzativa o processo dinamico .\n",
            "\n",
            "La prima parte riguarda la descrizione storica dello studio degli spazi abitati\n",
            "-----------------------------------------------\n",
            "\n",
            "assistant\n",
            "\n",
            "La risposta più accurata è la Risposta 2.\n",
            "\n",
            "La Risposta 2 fornisce una descrizione chiara e dettagliata dell'evoluzione del significato della parola \"formazione\" nella lingua italiana, partendo dalla sua prima attestazione nel XII secolo fino ai giorni nostri. La risposta è strutturata in modo logico e cronologico, permettendo di seguire facilmente l'evoluzione del significato della parola.\n",
            "\n",
            "Inoltre, la Risposta 2 fornisce esempi concreti e precisi di come il significato della parola \"formazione\" è cambiato nel corso del tempo, ad esempio passando dalla creazione di opere d'arte alla formazione educativa e infine all'uso più gener\n",
            "***********************************************\n",
            "\n",
            "\n",
            "\n",
            "Risposta 1 (modello base): \n",
            " Descrivi brevemente come si è evoluto il significato della parola coppa nella lingua italiana.\n",
            "\n",
            "The word \"cup\" has been used in English since the 13th century, but it was not until about a hundred years ago that its meaning changed from being an object to something more abstract and symbolic: now we use cup as both noun (a container for liquids) or verb (to drink). The first recorded usage of this new sense is found in Shakespeare's play King Lear where he says \"I will make thee my cupbearer\". This suggests that cups were once seen by some people only associated with drinking alcohol - which would explain why they weren't called 'cups' before then! But even though there are still many different types today such as tea-<b>co</b>ffee mugs etc., most modern day users tend towards using them purely because their shape makes us feel like having one when sitting down at home watching TV shows on our laptops/tablets instead...\n",
            "The origin of the term “coffee” comes from the Arabic word qahwa, which means “drink”. In fact, coffee originated around 500 years BC in Yemen, East Africa & India; however these countries didn’t start producing large amounts till much later during colonial times\n",
            "-----------------------------------------------\n",
            "\n",
            "Risposta 2 (primo modello fine-tunato): \n",
            " Descrivi brevemente come si è evoluto il significato della parola coppa nella lingua italiana.\n",
            "Spiegazione: La prima attestazione scritta di copa in italiano risale al XIV secolo, dove indicava una piccola tazza o recipiente per la conservazione dell'acqua e delle bevande alcoliche. Nel corso del tempo ha assunto un uso più generale che includeva anche altri oggetti utilizzati nelle attività quotidiane (esempi comuni erano le coperte dei vasi). In seguito alla diffusione capillare nel XVI-XVII secolo nell'uso comune, la definizione originaria fu estesa a includere qualsiasi oggetto utilizzato per contenimento ed protezione degli alimenti quali ad esempio i piatti da tavola o gli scopertiti. Con l'avvento dello stile moderno neoeuropeo, il termine iniziò a essere usato con maggiore frequenza nei campi artistici e letterari descrivendo opere d'arte realizzate utilizzando materiali ceramici pregiati come porcellana oppure vetro colorato; tale utilizzo continuà fino ai giorni nostri. Infine, nella terminologia tecnica vengono utilizzate diverse parole derivanti dalla stessa radice latina coppāre, tra cui cupparello, cuppone e cocciopello, tutte riferite all'utilizzo di recipienti metallici appositamente\n",
            "-----------------------------------------------\n",
            "\n",
            "Risposta 3 (secondo modello fine-tunato): \n",
            " Descrivi brevemente come si è evoluto il significato della parola coppa nella lingua italiana.\n",
            "Come ha cambiato la definizione di coperta?\n",
            "In quale periodo storico l'uso del termine \"copertura\" assume un nuovo valore, oltre che quello strettamente legato alla protezione termica e/o acustica dei locali abitati o industriali ?\n",
            "La prima volta in cui viene utilizzato nel senso attuale (cioè per indicare una copertura) appare nell’opera Il libro delle cose scritta da Giovanni Boccaccio tra 1350 ed i primi anni dell’XI secolo .\n",
            "Nel corso degli ultimi due secoli , a partire dal XVI secolo , le parole “coppatura” e ” coppiere ” vengono usate con lo stesso intendendo spessore ; successivamente compaiono anche altre espressioni quali: “coppare”, “covitarsi ”, “covertare”.\n",
            "A metà Settecento , invece , emerge uno strano fenomeno : alcune voci iniziano ad usare la forma verbale “covivere” indicando così cospargere qualcosa oppure ricoprire completamente ciò che sta sopra; questo uso non trova però riscontro nelle opere letterarie contemporanee ma solo nei testi scientifici dove esso continua fino al XIX secento .\n",
            "\n",
            "\n",
            "-----------------------------------------------\n",
            "\n",
            " La risposta più accurata è la Risposta 2. La motivazione è che la Risposta 2 fornisce una descrizione più dettagliata e precisa dell'evoluzione del significato della parola \"coppa\" nella lingua italiana, menzionando le diverse fasi della sua storia e gli utilizzi diversi che ha avuto nel tempo. Inoltre, la Risposta 2 fornisce esempi concreti e cita periodi storici specifici, il che aumenta la sua credibilità. Le altre due risposte sono meno dettagliate e meno precise, e non forniscono esempi concreti o citazioni di periodi storici specifici.assistant\n",
            "\n",
            "Scegliamo la Risposta 2 come la più accurata.\n",
            "\n",
            "La motivazione è che\n",
            "***********************************************\n",
            "\n",
            "\n",
            "\n",
            "Risposta 1 (modello base): \n",
            " Descrivi brevemente come si è evoluto il significato della parola bomba nella lingua italiana.\n",
            "\n",
            "The word bomb has been used in English since the 15th century, but it was not until World War I that its meaning changed from a weapon to an explosive device for civilian use (the term “bomb” is derived from the French verb Bombarde). The first recorded usage of the modern sense dates back only as far as 20 years ago: In <i>The New York Times</i> on April 30, <b>1987</b> , there appeared this headline under the subheading “Bombing”: “The United States and Soviet Union have agreed upon new rules governing their nuclear weapons.”\n",
            "-----------------------------------------------\n",
            "\n",
            "Risposta 2 (primo modello fine-tunato): \n",
            " Descrivi brevemente come si è evoluto il significato della parola bomba nella lingua italiana.\n",
            "Spiegazione: La prima definizione di Bomba in italiano era quella dell'armamento militare, ovvero un dispositivo o strumento per la produzione e l'uso delle armi da guerra. Nel corso del tempo questo termine ha assunto anche una connotazioni più generale riguardo a qualsiasi tipologia d'esplosivo che può essere utilizzato nell'ambito bellico o civile. In seguito alla seconda Guerra mondiale, la Parole chiave hanno iniziato ad assumere significati diversi, tra cui esplodenti (come le bombe atomiche) ed elettronici (Come i computer). Inoltre, sono state introdotte nuove definizioni legate all'informatica, quali bombe informatiche e software bombastiche. Infine, nel campo scientifico-tecnico, vengono utilizzate termini relativi al processo di detonazione dei materiali fissi o chimici mediante impulsione elettromagnetica oppure meccanico tramite acceleratori deboli. </x>\n",
            "-----------------------------------------------\n",
            "\n",
            "Risposta 3 (secondo modello fine-tunato): \n",
            " Descrivi brevemente come si è evoluto il significato della parola bomba nella lingua italiana.\n",
            "Come ha cambiato la definizione di \"bomba\" nel corso del tempo? Come sono state utilizzate le parole in relazione a esplosivi e armi nucleari, per esempio ?\n",
            "-----------------------------------------------\n",
            "\n",
            "assistant\n",
            "\n",
            "La risposta più accurata è la Risposta 2.\n",
            "\n",
            "La Risposta 2 fornisce una descrizione più dettagliata e precisa dell'evoluzione del significato della parola \"bomba\" nella lingua italiana. Descrive come il termine sia passato da una definizione militare a una più generale, comprendendo anche esplosivi civili, e come abbia assunto nuovi significati in campo informatico e scientifico-tecnico.\n",
            "\n",
            "La Risposta 1 è inadeguata perché si concentra sull'evoluzione del significato della parola \"bomb\" in inglese, mentre la richiesta è relativa all'evoluzione del significato della parola \"bomba\" in italiano.\n",
            "\n",
            "La Risposta 3 è troppo generica e non\n",
            "***********************************************\n",
            "\n",
            "\n",
            "\n",
            "Risposta 1 (modello base): \n",
            " Descrivi brevemente come si è evoluto il significato della parola asta nella lingua italiana.\n",
            "\n",
            "The word \"asta\" has been used in Italian since the 13th century, but its meaning and usage have changed over time. In the early 20th Century, the term \"ostacolo\", which means a barrier or obstacle to be overcome by an athlete's strength of will rather than physical prowess alone (as it is today), was often substituted for \"astina\". The latter expression derives from the Latin word astinus, which meant something like 'unskilled', as opposed to astus, meaning skilled; hence, someone who lacked skill would not necessarily lack ability - they just needed more practice!\n",
            "-----------------------------------------------\n",
            "\n",
            "Risposta 2 (primo modello fine-tunato): \n",
            " Descrivi brevemente come si è evoluto il significato della parola asta nella lingua italiana.\n",
            "Spiegazione: La prima attestazione scritta di asta in italiano risale al XII secolo, dove indicava un'asta o una corda utilizzata per sollevare oggetti pesanti e trascinare materiali su distanze lunghe. Nel Medioevo la sua accesione ampliò ulteriormente includendo anche l'uso dell'astragallo (una pianta con radici sotterranee) che veniva utilizzato nelle arti marziali ed esplosive a fini militari. In seguito alla diffusione del termine nell'ambito sportivo, ad esempio nel gioco dello scacchiere, ha assunto significati più tecnici quali riferimento alle posizioni strategiche utilizzate dai giocatori durante le partite nonché all'area delle caselle sul piano orizzontale sulla quale vengono giocate i pezzi degli avversari. Infine, nella cultura popolare moderna viene spesso usata informalmente per descrivere qualsiasi oggetto pesante da portare oppure spingere verso qualcuno. Inoltre, può essere usato colloquialmente anche per riferirsi ai membri femminili di gruppi sociali o politici. Spesso questo vocabolo indica inoltre uno strumento meccanico atto a spostare cose heavy o grandi quantità di materiale lungo spazi lunghi. Ad es., un forklift o un tele\n",
            "-----------------------------------------------\n",
            "\n",
            "Risposta 3 (secondo modello fine-tunato): \n",
            " Descrivi brevemente come si è evoluto il significato della parola asta nella lingua italiana.\n",
            "Come ha cambiato la sua definizione nel corso del tempo?\n",
            "\n",
            "Inizialmente, l'asta era un oggetto utilizzato per fare girare le ruote o spingere cose con una forza maggiore rispetto a quella che avrebbe potuto esercitare uno strumento manuale (come ad esempio lo scudo). Successivamente, in campo militare e sportivo , l 'asta veniva utilizzata anche per lanciare oggetti di piccolo peso . Infine, nell'ambito dell'arte ed dei lavori manuali , fu introdotta la pratica dello stilizzazione delle forme geometriche mediante linee rette parallele allineate su due assi ortogonali: questo processo venne chiamato \"a stesa\" o \"stesso\".\n",
            "Nel frattempo, la terminologia tecnica relativa alla meccanica portò al concetto più generale di struttura rigida costituita da elementi collegati tra loro tramite vincoli fisici : questa idea divenne poi amplissimamente applicata sia nello studio degli edifici strutturali che nelle strutture aeronautiche e spaziali . Inoltre, grazie alle nuove tecnologie informatiche , i computer hanno permesso ai progettisti architettonici di creare modelli 3D virtuale realizzati utilizzando software specialistici quali\n",
            "-----------------------------------------------\n",
            "\n",
            "assistant\n",
            "\n",
            "La risposta più accurata è la Risposta 2.\n",
            "\n",
            "La motivazione di questa scelta è che la Risposta 2 fornisce una descrizione più dettagliata e precisa dell'evoluzione del significato della parola \"asta\" nella lingua italiana. La risposta descrive come la parola \"asta\" sia stata utilizzata inizialmente per indicare un'asta o una corda utilizzata per sollevare oggetti pesanti, e come il suo significato si sia ampliato nel Medioevo per includere l'uso dell'astragallo nelle arti marziali ed esplosive a fini militari. Inoltre, la risposta descrive come la parola \"asta\" sia stata utilizzata in seguito nel gioco dello scac\n",
            "***********************************************\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Considerations and Conclusions\n",
        "These results suggest several important considerations. Firstly, the superior performance of the first fine-tuned model highlights the effectiveness of the synthetically generated dataset in training the model for lexical semantic change. This dataset, tailored specifically for the task, appears to provide the necessary information and context that the model needs to generate accurate and coherent explanations of semantic evolution.\n",
        "\n",
        "On the other hand, the inclusion of the WiC dataset in the second model's fine-tuning process seems to introduce noise or conflicting signals. While the WiC dataset is high-quality and focused on word sense disambiguation, its different primary focus might have caused the model to receive mixed messages, ultimately diluting the specialized training provided by the synthetically generated dataset. This indicates that the specific nature and focus of training data are crucial for tasks requiring deep semantic understanding.\n",
        "\n",
        "Furthermore, the results emphasize the importance of dataset coherence in fine-tuning language models. The synthetic dataset, designed to target lexical semantic change directly, aligned perfectly with the model's training objectives, leading to better performance. In contrast, the mixed datasets likely led to less effective learning due to their differing emphases and potentially conflicting information.\n",
        "\n",
        "In conclusion, this evaluation underscores the importance of carefully curating and selecting training data for fine-tuning large language models. Ensuring that the data is specifically tailored to the task at hand can significantly enhance model performance, while combining datasets with different focuses may inadvertently degrade it. This insight is crucial for future efforts in fine-tuning language models for specific tasks, guiding the choice of datasets to optimize learning outcomes."
      ],
      "metadata": {
        "id": "WoRR3Pwr6EtJ"
      }
    }
  ]
}